{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('..')\n",
    "\n",
    "import datetime as dt\n",
    "from capture_dou.inlabs_driver import InLabsDriver\n",
    "import pandas as pd\n",
    "\n",
    "#tipo_dou=\"DO1 DO2 DO3 DO1E DO2E DO3E\" # Seções separadas por espaço\n",
    "secoes = \"DO2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brasilia_day():\n",
    "    \"\"\"\n",
    "    No matter where the code is ran, return UTC-3 day\n",
    "    (Brasilia local day, no daylight savings)\n",
    "    \"\"\"\n",
    "    return (dt.datetime.utcnow() + dt.timedelta(hours=-3)).replace(hour=0, minute=0, second=0, microsecond=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import io\n",
    "import zipfile\n",
    "from lxml import etree\n",
    "\n",
    "def parse_zipped_response(response):\n",
    "    \"\"\"\n",
    "    Download a ZIP file and extract its contents in memory\n",
    "    yields (filename, file-like object) pairs\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract the contents of the .zip file in memory\n",
    "    zip_file = io.BytesIO(response.content)\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        for xml_file in zip_ref.namelist():\n",
    "            with zip_ref.open(xml_file) as f:\n",
    "                # Read the xml file from memory\n",
    "                xml_data = f.read()\n",
    "            # Parse the XML data\n",
    "            root = etree.fromstring(xml_data)\n",
    "            # Extract and parse the information you need from the XML data\n",
    "\n",
    "            return root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINAL CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joao/anaconda3/envs/scraping/lib/python3.9/site-packages/urllib3/connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'inlabs.in.gov.br'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/home/joao/anaconda3/envs/scraping/lib/python3.9/site-packages/urllib3/connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'inlabs.in.gov.br'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/home/joao/anaconda3/envs/scraping/lib/python3.9/site-packages/urllib3/connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'inlabs.in.gov.br'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/home/joao/anaconda3/envs/scraping/lib/python3.9/site-packages/urllib3/connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'inlabs.in.gov.br'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Inicialização do driver:\n",
    "driver = InLabsDriver()\n",
    "driver.login()\n",
    "\n",
    "# Montagem da URL:\n",
    "do_date_format = '%Y-%m-%d'\n",
    "# Transforms date to DOU format:\n",
    "date_string    = '2023-01-13'\n",
    "\n",
    "for dou_secao in secoes.split(' '):\n",
    "    file_url = driver.url_download + date_string + \"&dl=\" + date_string + \"-\" + dou_secao + \".zip\"\n",
    "    file_header = {'Cookie': 'inlabs_session_cookie=' + driver.cookie, 'origem': '736372697074'}\n",
    "    file_response = driver.session.request(\"GET\", file_url, headers = file_header)\n",
    "    if file_response.status_code == 200:\n",
    "        response = parse_zipped_response(file_response)\n",
    "        del file_response\n",
    "\n",
    "    elif file_response.status_code == 404:\n",
    "        print(\"File not found: %s\" % (date_string + \"-\" + dou_secao + \".zip\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PARSE_DOU_ARTICLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "def branch_text(branch):\n",
    "    \"\"\"\n",
    "    Takes and lxml tree element 'branch' and returns its text, \n",
    "    joined to its children's tails (i.e. the content that follows \n",
    "    childrens of 'branch').\n",
    "    \"\"\"\n",
    "    texts = list(filter(lambda s: s != None, [branch.text] + [child.tail for child in branch]))\n",
    "    if len(texts) == 0:\n",
    "        return None\n",
    "    text  = ' | '.join(texts)\n",
    "    return text\n",
    "\n",
    "\n",
    "def add_to_data(branch, data, key):\n",
    "    \"\"\"\n",
    "    Given a dict 'data' and a key, add the text (see branch_text function) \n",
    "    found in a branch to its value.\n",
    "    \"\"\"\n",
    "    if key in data:\n",
    "        if data[key] is None:\n",
    "            data[key] = branch_text(branch)\n",
    "        else:\n",
    "            data[key] = data[key] + ' | %s' % branch_text(branch)            \n",
    "    else:\n",
    "        data[key] = branch_text(branch)        \n",
    "    return data\n",
    "\n",
    "\n",
    "def recurse_over_nodes(tree, parent_key, data):\n",
    "    \"\"\"\n",
    "    Recursevely gets the text of the xml leafs and saves\n",
    "    its classes and keys and text as values\n",
    "    \n",
    "    input: \n",
    "        tree: lxml.etree._Element\n",
    "        parent_key: lxml.etree._Element\n",
    "        data: dict\n",
    "    return: dict\n",
    "    \"\"\"            \n",
    "    for branch in tree:\n",
    "        try:\n",
    "            key = branch.attrib.get('class')\n",
    "        except:\n",
    "            key = branch.attrib.get('id')\n",
    "        \n",
    "        if list(branch):\n",
    "            if parent_key:\n",
    "                key = '%s_%s' % (parent_key, key)    \n",
    "            add_to_data(branch, data, key) \n",
    "            data = recurse_over_nodes(branch, key, data)\n",
    "        \n",
    "        else:            \n",
    "            if parent_key:\n",
    "                key = '%s_%s' % (parent_key, key)            \n",
    "            add_to_data(branch, data, key)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def parse_xml_flattened(element, parent_key=\"\"):\n",
    "    result = {}\n",
    "    for child in element:\n",
    "        key = parent_key + child.tag\n",
    "        if len(child) > 0:\n",
    "            result.update(parse_xml_flattened(child, key + \"-\"))\n",
    "        else:\n",
    "            result[key] = child.text\n",
    "    return result\n",
    "\n",
    "\n",
    "def extract_necessary_fields(article):\n",
    "    \"\"\" \n",
    "    Extract necessary fields that were present in html parser but\n",
    "    are not present as keys in inlabs xml response\n",
    "    \"\"\"\n",
    "    orgao_dou_data = article.xpath('//article/@artCategory')[0]\n",
    "    edicao_dou_data =  article.xpath('//article/@editionNumber')[0]\n",
    "    secao_dou_data = article.xpath('//article/@numberPage')[0]\n",
    "    secao_dou = article.xpath('//article/@pubName')[0]\n",
    "    publicado_dou_data = article.xpath('//article/@pubDate')[0]\n",
    "\n",
    "    # Compile a regular expression pattern to search for the \"assina\" class\n",
    "    pattern = re.compile(r'<p class=\"assina\">(.*?)</p>')\n",
    "    # Search for the pattern in the assina variable\n",
    "    match = pattern.search(article.xpath('.//article/body/Texto/text()')[0])\n",
    "    # Extract the text within the <p> tags\n",
    "    if match:\n",
    "        assina_text = match.group(1)\n",
    "        assina = assina_text\n",
    "    else:\n",
    "        assina = ''\n",
    "        \n",
    "    return {'orgao-dou-data': orgao_dou_data, 'edicao-dou-data': edicao_dou_data, \n",
    "            'secao-dou-data': secao_dou_data, 'assina': assina, 'secao-dou': secao_dou,\n",
    "            'publicado-dou-data': publicado_dou_data}\n",
    "\n",
    "def filter_keys(data):\n",
    "    \"\"\"\n",
    "    Filter keys paths to get only last class from html\n",
    "    \n",
    "    input:\n",
    "        data: dict\n",
    "    return: dict\n",
    "    \"\"\"\n",
    "\n",
    "    final = defaultdict(lambda: '')\n",
    "\n",
    "    for k, v in data.items():\n",
    "        if v is not None:            \n",
    "            k_new = k.split('_')[-1]\n",
    "            final[k_new] =  ' | '.join([final[k_new], v]) if len(final[k_new]) > 0 else v\n",
    "            \n",
    "    return final\n",
    "\n",
    "\n",
    "def filter_values(data):\n",
    "    \"\"\"\n",
    "    Filter values that do not have letters or numbers\n",
    "    \n",
    "    input:\n",
    "        data: dict\n",
    "    return: dict\n",
    "    \"\"\"    \n",
    "    final = {}\n",
    "    \n",
    "    for k, v in data.items():        \n",
    "        if re.search('[a-zA-Z0-9]', v):        \n",
    "            final[k] = v\n",
    "            \n",
    "    return final    \n",
    "\n",
    "\n",
    "def decode(data, encoding= 'iso-8859-1', decoding='utf8'):\n",
    "    \"\"\"\n",
    "    Change enconding from string with secure error handling\n",
    "    \n",
    "    input:\n",
    "        data: dict\n",
    "        encoding: string\n",
    "        decoding: string\n",
    "    return: dict\n",
    "    \"\"\"    \n",
    "    final = {}\n",
    "    \n",
    "    for k, v in data.items():        \n",
    "        try:\n",
    "            final[k] = v.encode('iso-8859-1').decode('utf8')\n",
    "        except Exception as e:\n",
    "            print(\"Error\", e)\n",
    "            final[k] = v\n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_schema(key, value, url, url_certificado):\n",
    "    \"\"\"\n",
    "    Final data schema\n",
    "    \n",
    "    input:\n",
    "        key: string\n",
    "        value: string\n",
    "        url: string\n",
    "        url_certificado: string\n",
    "    return: dict\n",
    "    \"\"\"    \n",
    "    return {\n",
    "        \"key\": key,\n",
    "        \"value\": value,\n",
    "        \"url\": url,\n",
    "        \"capture_date\": datetime.strftime(datetime.now(), '%Y-%m-%d %H:%M:%S'),\n",
    "        \"url_certificado\": url_certificado\n",
    "    }\n",
    "\n",
    "def get_url_certificado(article):\n",
    "    \"\"\"\n",
    "    Gets the certified url in the xml\n",
    "    \n",
    "    input: \n",
    "        article: lxml.etree._Element\n",
    "    return: string\n",
    "    \"\"\"\n",
    "    return article.xpath('//article/@pdfPage')[0]\n",
    "\n",
    "def get_data(article):\n",
    "    \"\"\"\n",
    "    Get relevant data from xml. It recursevely gets leaf text from xml\n",
    "    and saves theirs classes as keys. \n",
    "    It also creates an item in dict's key 'full-text' with all text \n",
    "    in the xml, without tags.\n",
    "    \n",
    "    input: \n",
    "        article: lxml.etree._Element\n",
    "    return: dict\n",
    "    \"\"\"\n",
    "    data = parse_xml_flattened(article)\n",
    "\n",
    "    # filtra None e melhora keys\n",
    "    data = filter_keys(data)\n",
    "    data = filter_values(data)\n",
    "    data = {k: v for k,v in data.items() if len(k) != 0}\n",
    "\n",
    "    # Include other fields from extraction\n",
    "    fields = extract_necessary_fields(article)\n",
    "    data['orgao-dou-data'] = fields['orgao-dou-data']\n",
    "    data['edicao-dou-data'] = fields['edicao-dou-data']\n",
    "    data['secao-dou-data'] = fields['secao-dou-data']\n",
    "    data['assina'] = fields['assina']\n",
    "    data['secao-dou'] = fields['secao-dou']\n",
    "    data['publicado-dou-data'] = fields['publicado-dou-data']\n",
    "\n",
    "    # Include full-text ignoring xml tags and formatters:\n",
    "    full_text = etree.tostring(article, pretty_print=True, encoding='unicode')\n",
    "    full_text = ' '.join(full_text.split())\n",
    "    full_text = re.search(r'<Texto>(.*?)</Texto>', full_text).group(1)\n",
    "    full_text = full_text.replace('&lt;','<').replace('&gt;','>')\n",
    "    clean_full_text = re.sub(r'<[^>]+>', '', full_text)\n",
    "\n",
    "    data['fulltext'] = clean_full_text\n",
    "\n",
    "    return data\n",
    "\n",
    "def structure_data(data, url, article):\n",
    "    \"\"\"\n",
    "    Structures html parsed data to list of dicts \n",
    "    ready to be processed by http-request Lambda\n",
    "    Function.\n",
    "    It adds the capture date, url, and \n",
    "    certified url\n",
    "    \n",
    "    input: \n",
    "        data: dict\n",
    "        url: string\n",
    "        artigo: lxml.html.HtmlElement\n",
    "    return: list of dict\n",
    "    \"\"\"    \n",
    "    url_certificado = get_url_certificado(article)\n",
    "    \n",
    "    final = []\n",
    "    for key, value in data.items():        \n",
    "        final.append(data_schema(key, value, url, url_certificado))\n",
    "        \n",
    "    return final\n",
    "        \n",
    "\n",
    "def parse_dou_article(response, url=''):\n",
    "    \"\"\"\n",
    "    Gets an HTTP request response for a DOU article's URL and that url \n",
    "    and parse the relevant fields to a list of dicts. Each dict has the \n",
    "    keys: \n",
    "    * key             -- an html tag class identifying the field;\n",
    "    * value           -- the respective value (text) in that field;\n",
    "    * url             -- The original article URL;\n",
    "    * capture_date    -- The date when capture occured;\n",
    "    * url_certificado -- The link to the certified version of the article.\n",
    "    \"\"\"\n",
    "    data    = get_data(response)    \n",
    "    data    = structure_data(data, url, response)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STRUCTURE_ARTICLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key_value(key, article_raw):\n",
    "    \"\"\"\n",
    "    Searches for an entry in article_raw (which is a list of dicts) that\n",
    "    has the 'key'. Then it returns the value associated to that key. \n",
    "    If the key is not found, return None.\n",
    "    \"\"\" \n",
    "    sel = list(filter(lambda d: d['key']==key, article_raw))\n",
    "    if len(sel)==0:\n",
    "        return None\n",
    "    return sel[0]['value']\n",
    "\n",
    "\n",
    "def make_resumo(fulltext):\n",
    "    \"\"\"\n",
    "    Given a string (fulltext), this function aims to extract \n",
    "    the most important part of it as a abstract.\n",
    "    \"\"\"\n",
    "\n",
    "    # Termos a serem pesquisados:\n",
    "    termos = ['resolve:', 'onde se l', 'objeto:', 'espécie']\n",
    "    # Tamanho do resumo:\n",
    "    resumo_size = 300\n",
    "    \n",
    "    # Alterando o texto para minúsculo    \n",
    "    fulltext  = str(fulltext)\n",
    "    paragraph = fulltext.lower()\n",
    "         \n",
    "    for termo in termos:\n",
    "        \n",
    "        pos = paragraph.find(termo)\n",
    "        \n",
    "        if pos != -1: \n",
    "            # Se encontra algum dos termos, resume o texto com os 300 primeiros caracteres \n",
    "            # a partir do termo encontrado.\n",
    "            abstract = fulltext[pos:pos + resumo_size]    \n",
    "            break            # O break aqui serve para garantir que, caso um termo seja encontrado, \n",
    "                             # não busque pelos demais.\n",
    "        \n",
    "    if pos == -1:\n",
    "            abstract = fulltext[:resumo_size]   # Se não encontra nenhum dos termos, resume o texto \n",
    "                                                # nos primeros 300 caracteres.          \n",
    "    \n",
    "    if len(fulltext[pos:]) > len(abstract):\n",
    "        abstract = abstract + '...'\n",
    "    \n",
    "    return abstract\n",
    "    \n",
    "def structure_paragraph(paragraph):\n",
    "    \"\"\"\n",
    "    Given a html text paragraph, this function aims to extract the clean text\n",
    "    ignoring the html tags.\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove tags\n",
    "    pattern = r'<p>(.*?)<\\/p>'\n",
    "    paragraph = ' '.join(re.findall(pattern, paragraph))\n",
    "\n",
    "    return paragraph\n",
    "\n",
    "def structure_article(article_raw):\n",
    "    \"\"\"\n",
    "    Takes a list of dicts that represent a DOU article with the keywords\n",
    "    key, value, capture_date, url and url_certificado and select relevant \n",
    "    keys (hard-coded), rename them and output a dict with only the relevant \n",
    "    keys.\n",
    "    \"\"\"\n",
    "\n",
    "    relevant_keys = ['secao-dou', 'orgao-dou-data', 'assina', 'article-body-Identifica', 'cargo', 'secao-dou-data', \n",
    "                     'edicao-dou-data', 'dou-em', 'ementa', 'dou-strong', 'titulo', 'subtitulo', \n",
    "                     'article-body-Texto', 'publicado-dou-data', 'assinaPr', 'fulltext']\n",
    "    new_keys      = ['secao', 'orgao', 'assina', 'identifica', 'cargo', 'pagina',\n",
    "                     'edicao', 'italico', 'ementa', 'strong', 'ato_orgao', 'subtitulo', \n",
    "                     'paragraph', 'pub_date', 'assinaPr', 'fulltext']\n",
    "    \n",
    "    relevant_values = [get_key_value(key, article_raw) for key in relevant_keys]\n",
    "    struct = dict(zip(new_keys, relevant_values))\n",
    "    \n",
    "    # Join with identifying fields:\n",
    "    struct['capture_date']    = article_raw[0]['capture_date']\n",
    "    struct['url']             = article_raw[0]['url']\n",
    "    struct['url_certificado'] = article_raw[0]['url_certificado']\n",
    "    \n",
    "    # Format selected fields:\n",
    "    struct['secao']  = re.search(r'\\d+(?:\\w)?', struct['secao']).group()\n",
    "    if struct['assinaPr'] != None:   # Existe assinatura do presidente.\n",
    "        if struct['assina'] != None: # Existe as duas assinaturas.\n",
    "            struct['assina'] = struct['assinaPr'] + ' | ' + struct['assina']\n",
    "        else:                        # Só existe a assinatura do presidente.\n",
    "            struct['assina'] = struct['assinaPr']\n",
    "\n",
    "    # Structure paragraph:\n",
    "    struct['paragraph'] = structure_paragraph(struct['paragraph'])\n",
    "\n",
    "    # Create new field (all the text):\n",
    "    fields_list = filter(lambda s: s!=None, [struct['ato_orgao'], struct['subtitulo'], struct['ementa'], \n",
    "                                            struct['strong'], struct['italico'], struct['paragraph']])\n",
    "    struct['alltext'] = ' | '.join(fields_list)\n",
    "    # Another new field (a clipping):\n",
    "    struct['resumo'] = make_resumo(struct['fulltext'])\n",
    "        \n",
    "    return struct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured = structure_article(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>secao</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orgao</th>\n",
       "      <td>Ministério Público da União/Ministério Público...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assina</th>\n",
       "      <td>MARCELO GOSS NEVES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>identifica</th>\n",
       "      <td>Portaria Nº 256, de 13 de dezembro de 2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cargo</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pagina</th>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>edicao</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>italico</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ementa</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>strong</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ato_orgao</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subtitulo</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paragraph</th>\n",
       "      <td>PGEA 20.02.1200.0001047/2022-18 O Procurador-C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pub_date</th>\n",
       "      <td>13/01/2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assinaPr</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fulltext</th>\n",
       "      <td>Portaria Nº 256, de 13 de dezembro de 2022PGEA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>capture_date</th>\n",
       "      <td>2023-01-19 14:47:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>url</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>url_certificado</th>\n",
       "      <td>http://pesquisa.in.gov.br/imprensa/jsp/visuali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alltext</th>\n",
       "      <td>PGEA 20.02.1200.0001047/2022-18 O Procurador-C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resumo</th>\n",
       "      <td>resolve:Art. 1º Dispensar, a servidora Flávia ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 0\n",
       "secao                                                            2\n",
       "orgao            Ministério Público da União/Ministério Público...\n",
       "assina                                          MARCELO GOSS NEVES\n",
       "identifica              Portaria Nº 256, de 13 de dezembro de 2022\n",
       "cargo                                                         None\n",
       "pagina                                                          46\n",
       "edicao                                                          10\n",
       "italico                                                       None\n",
       "ementa                                                        None\n",
       "strong                                                        None\n",
       "ato_orgao                                                     None\n",
       "subtitulo                                                     None\n",
       "paragraph        PGEA 20.02.1200.0001047/2022-18 O Procurador-C...\n",
       "pub_date                                                13/01/2023\n",
       "assinaPr                                                      None\n",
       "fulltext         Portaria Nº 256, de 13 de dezembro de 2022PGEA...\n",
       "capture_date                                   2023-01-19 14:47:43\n",
       "url                                                               \n",
       "url_certificado  http://pesquisa.in.gov.br/imprensa/jsp/visuali...\n",
       "alltext          PGEA 20.02.1200.0001047/2022-18 O Procurador-C...\n",
       "resumo           resolve:Art. 1º Dispensar, a servidora Flávia ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(structured, index=[0]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('scraping')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b57a106df503026757037a3564c5b06bb9bd0ebd4985fd2fe126eeda7bdee66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
