{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('..')\n",
    "\n",
    "import datetime as dt\n",
    "from capture_dou.inlabs_driver import InLabsDriver\n",
    "\n",
    "#tipo_dou=\"DO1 DO2 DO3 DO1E DO2E DO3E\" # Seções separadas por espaço\n",
    "secoes = \"DO2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brasilia_day():\n",
    "    \"\"\"\n",
    "    No matter where the code is ran, return UTC-3 day\n",
    "    (Brasilia local day, no daylight savings)\n",
    "    \"\"\"\n",
    "    return (dt.datetime.utcnow() + dt.timedelta(hours=-3)).replace(hour=0, minute=0, second=0, microsecond=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import io\n",
    "import zipfile\n",
    "from lxml import etree\n",
    "\n",
    "def parse_zipped_response(response):\n",
    "    \"\"\"\n",
    "    Download a ZIP file and extract its contents in memory\n",
    "    yields (filename, file-like object) pairs\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract the contents of the .zip file in memory\n",
    "    zip_file = io.BytesIO(response.content)\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        for xml_file in zip_ref.namelist():\n",
    "            with zip_ref.open(xml_file) as f:\n",
    "                # Read the xml file from memory\n",
    "                xml_data = f.read()\n",
    "            # Parse the XML data\n",
    "            root = etree.fromstring(xml_data)\n",
    "            # Extract and parse the information you need from the XML data\n",
    "\n",
    "            return root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PARSE_DOU_ARTICLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "def branch_text(branch):\n",
    "    \"\"\"\n",
    "    Takes and lxml tree element 'branch' and returns its text, \n",
    "    joined to its children's tails (i.e. the content that follows \n",
    "    childrens of 'branch').\n",
    "    \"\"\"\n",
    "    texts = list(filter(lambda s: s != None, [branch.text] + [child.tail for child in branch]))\n",
    "    if len(texts) == 0:\n",
    "        return None\n",
    "    text  = ' | '.join(texts)\n",
    "    return text\n",
    "\n",
    "\n",
    "def add_to_data(branch, data, key):\n",
    "    \"\"\"\n",
    "    Given a dict 'data' and a key, add the text (see branch_text function) \n",
    "    found in a branch to its value.\n",
    "    \"\"\"\n",
    "    if key in data:\n",
    "        if data[key] is None:\n",
    "            data[key] = branch_text(branch)\n",
    "        else:\n",
    "            data[key] = data[key] + ' | %s' % branch_text(branch)            \n",
    "    else:\n",
    "        data[key] = branch_text(branch)        \n",
    "    return data\n",
    "\n",
    "\n",
    "def recurse_over_nodes(tree, parent_key, data):\n",
    "    \"\"\"\n",
    "    Recursevely gets the text of the xml leafs and saves\n",
    "    its classes and keys and text as values\n",
    "    \n",
    "    input: \n",
    "        tree: lxml.etree._Element\n",
    "        parent_key: lxml.etree._Element\n",
    "        data: dict\n",
    "    return: dict\n",
    "    \"\"\"            \n",
    "    for branch in tree:\n",
    "        key = branch.attrib.get('class') or branch.attrib.get('id')\n",
    "        \n",
    "        if list(branch):\n",
    "            if parent_key:\n",
    "                key = '%s_%s' % (parent_key, key)    \n",
    "            add_to_data(branch, data, key) \n",
    "            data = recurse_over_nodes(branch, key, data)\n",
    "        \n",
    "        else:            \n",
    "            if parent_key:\n",
    "                key = '%s_%s' % (parent_key, key)            \n",
    "            add_to_data(branch, data, key)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def filter_keys(data):\n",
    "    \"\"\"\n",
    "    Filter keys paths to get only last class from html\n",
    "    \n",
    "    input:\n",
    "        data: dict\n",
    "    return: dict\n",
    "    \"\"\"\n",
    "\n",
    "    final = defaultdict(lambda: '')\n",
    "\n",
    "    for k, v in data.items():\n",
    "        if v is not None:            \n",
    "            k_new = k.split('_')[-1]\n",
    "            final[k_new] =  ' | '.join([final[k_new], v]) if len(final[k_new]) > 0 else v\n",
    "            \n",
    "    return final\n",
    "\n",
    "\n",
    "def filter_values(data):\n",
    "    \"\"\"\n",
    "    Filter values that do not have letters or numbers\n",
    "    \n",
    "    input:\n",
    "        data: dict\n",
    "    return: dict\n",
    "    \"\"\"    \n",
    "    final = {}\n",
    "    \n",
    "    for k, v in data.items():        \n",
    "        if re.search('[a-zA-Z0-9]', v):        \n",
    "            final[k] = v\n",
    "            \n",
    "    return final    \n",
    "\n",
    "\n",
    "def decode(data, encoding= 'iso-8859-1', decoding='utf8'):\n",
    "    \"\"\"\n",
    "    Change enconding from string with secure error handling\n",
    "    \n",
    "    input:\n",
    "        data: dict\n",
    "        encoding: string\n",
    "        decoding: string\n",
    "    return: dict\n",
    "    \"\"\"    \n",
    "    final = {}\n",
    "    \n",
    "    for k, v in data.items():        \n",
    "        try:\n",
    "            final[k] = v.encode('iso-8859-1').decode('utf8')\n",
    "        except Exception as e:\n",
    "            print(\"Error\", e)\n",
    "            final[k] = v\n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_schema(key, value, url, url_certificado):\n",
    "    \"\"\"\n",
    "    Final data schema\n",
    "    \n",
    "    input:\n",
    "        key: string\n",
    "        value: string\n",
    "        url: string\n",
    "        url_certificado: string\n",
    "    return: dict\n",
    "    \"\"\"    \n",
    "    return {\n",
    "        \"key\": key,\n",
    "        \"value\": value,\n",
    "        \"url\": url,\n",
    "        \"capture_date\": datetime.strftime(datetime.now(), '%Y-%m-%d %H:%M:%S'),\n",
    "        \"url_certificado\": url_certificado\n",
    "    }\n",
    "\n",
    "\n",
    "def get_url_certificado(article):\n",
    "    \"\"\"\n",
    "    Gets the certified url in the xml\n",
    "    \n",
    "    input: \n",
    "        article: lxml.etree._Element\n",
    "    return: string\n",
    "    \"\"\"\n",
    "    return article.xpath('//article/@pdfPage')[0]\n",
    "\n",
    "def get_data(article):\n",
    "    \"\"\"\n",
    "    Get relevant data from xml. It recursevely gets leaf text from xml\n",
    "    and saves theirs classes as keys. \n",
    "    It also creates an item in dict's key 'full-text' with all text \n",
    "    in the xml, without tags.\n",
    "    \n",
    "    input: \n",
    "        article: lxml.etree._Element\n",
    "    return: dict\n",
    "    \"\"\"\n",
    "    data = recurse_over_nodes(article, None, {})\n",
    "\n",
    "    # filtra None e melhora keys\n",
    "    data = filter_keys(data)\n",
    "    data = filter_values(data)\n",
    "    data = {k: v for k,v in data.items() if len(k) != 0}\n",
    "\n",
    "    # Include full-text:\n",
    "    data['fulltext'] = article.text\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def structure_data(data, url, article):\n",
    "    \"\"\"\n",
    "    Structures html parsed data to list of dicts \n",
    "    ready to be processed by http-request Lambda\n",
    "    Function.\n",
    "    It adds the capture date, url, and \n",
    "    certified url\n",
    "    \n",
    "    input: \n",
    "        data: dict\n",
    "        url: string\n",
    "        artigo: lxml.html.HtmlElement\n",
    "    return: list of dict\n",
    "    \"\"\"    \n",
    "    url_certificado = get_url_certificado(article)\n",
    "    \n",
    "    final = []\n",
    "    for key, value in data.items():        \n",
    "        final.append(data_schema(key, value, url, url_certificado))\n",
    "        \n",
    "    return final\n",
    "        \n",
    "\n",
    "def parse_dou_article(response, url=''):\n",
    "    \"\"\"\n",
    "    Gets an HTTP request response for a DOU article's URL and that url \n",
    "    and parse the relevant fields to a list of dicts. Each dict has the \n",
    "    keys: \n",
    "    * key             -- an html tag class identifying the field;\n",
    "    * value           -- the respective value (text) in that field;\n",
    "    * url             -- The original article URL;\n",
    "    * capture_date    -- The date when capture occured;\n",
    "    * url_certificado -- The link to the certified version of the article.\n",
    "    \"\"\"\n",
    "    data    = get_data(response)    \n",
    "    data    = structure_data(data, url, response)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<xml>\\n  <article id=\"30079829\" name=\"Portaria 256.2022 dispensa FlAvi\" idOficio=\"9288474\" pubName=\"DO2\" artType=\"Portaria\" pubDate=\"13/01/2023\" artClass=\"00043:00012:00013:00000:00000:00000:00000:00000:00000:00000:00017:00000\" artCategory=\"Ministério Público da União/Ministério Público do Trabalho/Procuradoria Regional do Trabalho da 12ª Região\" artSize=\"12\" artNotes=\"\" numberPage=\"46\" pdfPage=\"http://pesquisa.in.gov.br/imprensa/jsp/visualiza/index.jsp?data=13/01/2023&amp;jornal=529&amp;pagina=46\" editionNumber=\"10\" highlightType=\"\" highlightPriority=\"\" highlight=\"\" highlightimage=\"\" highlightimagename=\"\" idMateria=\"20165078\">\\n  <body>\\n    <Identifica> Portaria Nº 256, de 13 de dezembro de 2022</Identifica>\\n    <Data></Data>\\n    <Ementa/>\\n    <Titulo/>\\n    <SubTitulo/>\\n    <Texto>&lt;p class=\"identifica\"&gt;Portaria Nº 256, de 13 de dezembro de 2022&lt;/p&gt;&lt;p&gt;PGEA 20.02.1200.0001047/2022-18&lt;/p&gt;&lt;p&gt;O Procurador-Chefe da Procuradoria Regional do Trabalho da 12ª Região, no uso de suas atribuições institucionais e considerando a Portaria PGT nº 1728, de 02 de outubro de 2017, resolve:&lt;/p&gt;&lt;p&gt;Art. 1º Dispensar, a servidora Flávia Carolina Postalli Rodrigues Alloy, matrícula 6007731-X, do encargo de substituta eventual da Chefe da Assessoria Jurídica do 14º Ofício Geral da PRT 12ª Região, código CC-02.&lt;/p&gt;&lt;p&gt;Art. 2º Designar a servidora Rosmari Rudolf Mezzomo, matrícula 6002253-1 para o encargo de substituta eventual do Chefe da Assessoria Jurídica do 14º Ofício Geral da PRT 12ª Região, código CC-02.&lt;/p&gt;&lt;p class=\"assina\"&gt;MARCELO GOSS NEVES&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</Texto>\\n  </body>\\n  <Midias/>\\n</article>\\n</xml>\\n'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etree.tostring(data, pretty_print=True, encoding='unicode', with_tail=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Element article at 0x7f0b1bf1f900>\n",
      "30079829\n"
     ]
    }
   ],
   "source": [
    "tree = data\n",
    "parent_key = None\n",
    "data2 = {}\n",
    "for branch in tree:\n",
    "    print(branch)\n",
    "    key = branch.attrib.get('class') or branch.attrib.get('id')\n",
    "    if list(branch):\n",
    "        print(key)\n",
    "        if parent_key:\n",
    "            key = '%s_%s' % (parent_key, key)    \n",
    "        add_to_data(branch, data2, key) \n",
    "        data2 = recurse_over_nodes(branch, key, data2)\n",
    "    \n",
    "    else:            \n",
    "        if parent_key:\n",
    "            key = '%s_%s' % (parent_key, key)            \n",
    "        add_to_data(branch, data2, key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'30079829': '\\n   | \\n   | \\n',\n",
       " '30079829_None': '\\n     | \\n     | \\n     | \\n     | \\n     | \\n     | \\n   | None',\n",
       " '30079829_None_None': ' Portaria Nº 256, de 13 de dezembro de 2022 |  | None | None | None | <p class=\"identifica\">Portaria Nº 256, de 13 de dezembro de 2022</p><p>PGEA 20.02.1200.0001047/2022-18</p><p>O Procurador-Chefe da Procuradoria Regional do Trabalho da 12ª Região, no uso de suas atribuições institucionais e considerando a Portaria PGT nº 1728, de 02 de outubro de 2017, resolve:</p><p>Art. 1º Dispensar, a servidora Flávia Carolina Postalli Rodrigues Alloy, matrícula 6007731-X, do encargo de substituta eventual da Chefe da Assessoria Jurídica do 14º Ofício Geral da PRT 12ª Região, código CC-02.</p><p>Art. 2º Designar a servidora Rosmari Rudolf Mezzomo, matrícula 6002253-1 para o encargo de substituta eventual do Chefe da Assessoria Jurídica do 14º Ofício Geral da PRT 12ª Região, código CC-02.</p><p class=\"assina\">MARCELO GOSS NEVES</p><p></p>'}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINAL CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joao/anaconda3/envs/scraping/lib/python3.9/site-packages/urllib3/connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'inlabs.in.gov.br'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/home/joao/anaconda3/envs/scraping/lib/python3.9/site-packages/urllib3/connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'inlabs.in.gov.br'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/home/joao/anaconda3/envs/scraping/lib/python3.9/site-packages/urllib3/connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'inlabs.in.gov.br'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/home/joao/anaconda3/envs/scraping/lib/python3.9/site-packages/urllib3/connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'inlabs.in.gov.br'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Inicialização do driver:\n",
    "driver = InLabsDriver()\n",
    "driver.login()\n",
    "\n",
    "# Montagem da URL:\n",
    "do_date_format = '%Y-%m-%d'\n",
    "# Transforms date to DOU format:\n",
    "date_string    = brasilia_day().strftime(do_date_format)\n",
    "\n",
    "for dou_secao in secoes.split(' '):\n",
    "    file_url = driver.url_download + date_string + \"&dl=\" + date_string + \"-\" + dou_secao + \".zip\"\n",
    "    file_header = {'Cookie': 'inlabs_session_cookie=' + driver.cookie, 'origem': '736372697074'}\n",
    "    file_response = driver.session.request(\"GET\", file_url, headers = file_header)\n",
    "    if file_response.status_code == 200:\n",
    "        data = parse_zipped_response(file_response)\n",
    "        del file_response\n",
    "\n",
    "    elif file_response.status_code == 404:\n",
    "        print(\"File not found: %s\" % (date_string + \"-\" + dou_secao + \".zip\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('scraping')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b57a106df503026757037a3564c5b06bb9bd0ebd4985fd2fe126eeda7bdee66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
